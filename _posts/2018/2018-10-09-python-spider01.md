---
layout: single
author_profile: true
title: "python爬虫入门-01"
date: 2018-10-09 17:30:53
# toc: true
tags:
  - python
  - 爬虫
categories:
  - python
  - python爬虫
---

### 什么是爬虫

我们先来看看百度百科的定义:

网络爬虫（又被称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。

简单的说爬虫就是先把目标网页的内容拉回来，再把需要的信息解析出来的一个工具程序.

### 为什么学习爬虫？

看到这里，有人就要问了：google、百度等搜索引擎已经帮我们抓取了互联网上的大部分信息了，为什么还要自己写爬虫呢？这是因为，需求是多样的。比如在企业中，爬取下来的数据可以作为数据挖掘的数据源。甚至有人为了炒股，专门抓取股票信息。笔者就见过有人为了分析房价，自学编程，爬了绿中介的数据。

在大数据深入人心的时代，网络爬虫作为网络、存储与机器学习等领域的交汇点，已经成为满足个性化网络数据需求的最佳实践。


### 用到的技术

编程语言: python3.x

库：
requests: 这是一个封装了http请求的第三方库， 用起来非常简单， 比python自带的要好用。 因此推荐。 安装方法: pip3 install requests
lxml: 是一个可以从HTML或XML文件中提取数据的Python库。它能够通过你喜欢的转换器实现惯用的文档导航，查找，修改文档的方式。(还有一个叫做BeautifulSoup的库也可以完成同样的同坐， 使用方法类似，有兴趣的读者可以自行安装实验一下) 安装方法: pip3 install lxml

### 爬取数据

接下来，我们就用requests获取html内容，再用lxml提取其中的数据，完成一次简单的爬取

```
import requests

response = requests.get('http://jr.jd.com')
print(response.content)
```

spider1.py，然后运行，看看输出了什么
```
b'<!DOCTYPE html>\n<html>\n<head>\n    <meta charset = "utf-8">\n    <meta http-equiv = "Content-Type" content = "text/html; charset=utf-8">\n    <meta name="description" content = "\xe4\xba\xac\xe4\xb8\x9c\xe9\x87\x91\xe8\x9e\x8d\xe5\xae\x98\xe7\xbd\x91\xef\xbc\x8c\xe6\x9c\x8d\xe5\x8a\xa1\xe9\x87\x91\xe8\x9e\x8d\xe6\x9c\xba\xe6\x9e\x84\xe7\x9a\x84\xe6\x95\xb0\xe5\xad\x97\xe7\xa7\x91\xe6\x8a\x80\xe5\x85\xac\xe5\x8f\xb8\xe3\x80\x82\xe4\xb8\xad\xe5\x9b\xbd\xe4\xba\x92\xe8\x81\x94\xe7\xbd\x91\xe9\x87\x91\xe8\x9e\x8d\xe5\x8d\x8f\xe4\xbc\x9a\xe7\x90\x86\xe4\xba\x8b\xe5\x8d\x95\xe4\xbd\x8d! \xe5\x8f\x82\xe4\xb8\x8e\xe4\xb8\xad\xe5\xa4\xae\xe7\xbd\x91\xe4\xbf\xa1\xe5\x8a\x9e\xe7\xad\x89\xe5\x9b\x9b\xe9\x83\xa8\xe5\xa7\x94\xe5\x8f\x91\xe8\xb5\xb7\xe7\x9a\x84\xe8\x81\x94\xe5\x90\x88\xe5\xae\x89\xe5\x85\xa8\xe6\xb5\x8b\xe8\xaf\x84\xe5\xb9\xb6\xe4\xbd\x8d\xe5\xb1\x85\xe6\xa6\x9c\xe9\xa6\x96\xe3\x80\x82\xe6\x97\x97\xe4\xb8\x8b\xe5\x93\x81\xe7\x89\x8c\xe5\x8c\x85\xe6\x8b\xac\xe4\xba\xac\xe4\xb8\x9c\xe8\xb4\xa2\xe5\xaf\x8c\xe3\x80\x81\xe4\xba\xac\xe4\xb8\x9c\xe4\xbc\x97\xe7\xad\xb9\xe3\x80\x81\xe4\xba\xac\xe4\xb8\x9c\xe4\xbf\x9d\xe9\x99\xa9\xe3\x80\x81\xe4\xba\xac\xe4\xb8\x9c\xe7\x99\xbd\xe6\x9d\xa1\xe3\x80\x81\xe4\xbc\x81\xe4\xb8\x9a\xe9\x87\x91\xe8\x9e\x8d\xe3\x80\x81\xe4\xba\xac\xe4\xb8\x9c\xe8\x82\xa1\xe7\xa5\xa8\xe3\x80\x81\xe4\xb8\x9c\xe5\xae\xb6\xe8\xb4\xa2\xe5\xaf\x8c\xe3\x80\x81\xe9\x87\x91\xe8\x9e\x8d\xe4\xba\x91\xe3\x80\x81\xe5\x9f\x8e\xe5\xb8\x82\xe8\xae\xa1\xe7\xae\x97\xe7\xad\x89\xe3\x80\x82">\n    <meta name="Keywords" content = "\xe4\xba\x92\xe8\x81\x94\xe7\xbd\x91\xe9\x87\x91\xe8\x9e\x8d,\xe9\x87\x91\xe8\x9e\x8d\xe6\x9c\xba\xe6\x9e\x84,\xe6\x95\xb0\xe5\xad\x97\xe7\xa7\x91\xe6\x8a\x80\xe5\x85\xac\xe5\x8f\xb8,\xe8\xb5\x9a\xe9\x92\xb1,\xe5\x80\x9f\xe9\x92\xb1,\xe8\x8a\xb1\xe9\x92\xb1,\xe4\xba\xac\xe4\xb8\x9c\xe9\x87\x91\xe8\x9e\x8d"/>\n    <meta http-equiv = "X-UA-Compatible" content = "IE=Edge,chrome=1">\n    <meta name = "renderer" content = "webkit">\n    <link rel = "dns-prefetch" href = "//static.360buyimg.com">\n    <link rel = "dns-prefetch" href = "//img30.360buyimg.com">\n    <title>\xe4\xba\xac\xe4\xb8\x9c\xe9\x87\x91\xe8\x9e\x8d-\xe6\x9c\x8d\xe5\x8a\xa1\xe9\x87\x91\xe8\x9e\x8d\xe6\x9c\xba\xe6\x9e\x84\xe7\x9a\x84\xe6\x95\xb0\xe5\xad\x97\xe7\xa7\x91\xe6\x8a\x80\xe5\x85\xac\xe5\x8f\xb8</title>\n    <link rel="icon" href ="//www.jd.com/favicon.ico" type="image/x-icon">\n...'
```


果然，输出了http://jr.jd.com 这个网页的全部HTML代码。

输出的代码简直无法直视，如何方便的找到我们想抓取数据呢？用Chrome打开url，然后按F12，再按Ctrl + Shift + C。如果我们要抓导航栏，就用鼠标点击任意一个导航栏的项目，浏览器就在html中找到了它的位置。效果如下：

![](/assets/images/spider/spider1.png)

定位到html代码
```
<a class="nav-item-primary" href="//bao.jd.com/" clstag="jr|keycount|jr_shouye|baoxian">保险
  <i></i>
  <b></b>
</a>
```


有了这些信息，就可以用lxml提取数据了。升级一下代码：
```
import requests
import lxml.html as html

response = requests.get('http://jr.jd.com')  # 拿到网页代码

# 把网页代码转换成对象
page = html.fromstring(response.content.decode())
for a in page.xpath("//a[@class='nav-item-primary']"):  # 这里运用了xpath的语法
    print(a.text_content())

# 关于xpath的语法可以参考: http://www.w3school.com.cn/xpath/index.asp
```

运行代码，看看输出了什么:
```
首页
财富
众筹
保险
白条
股票
东家财富
企业金融
金融云
城市计算
```

没错，得到了我们想要的数据！

lxml提供一些简单的、Python式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。怎么样，是不是觉得只要复制粘贴就可以写爬虫了？简单的爬虫确实是可以的！